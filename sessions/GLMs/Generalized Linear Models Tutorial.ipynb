{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Model's\n",
    "\n",
    "What are GLMs? In short: a generalization of **linear regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick recap of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create artificial data\n",
    "f = lambda x: 0.2*x - 1\n",
    "x = np.linspace(0, 10, 100)   # collect 100 data points\n",
    "e = np.random.normal(loc=0, scale=0.3, size=(100))\n",
    "y = f(x) + e\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = sm.add_constant(x)\n",
    "model = sm.GLM(y, X, family=sm.families.Gaussian())\n",
    "fit = model.fit()\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "abline_plot(model_results=fit, ax=plt.gca())\n",
    "plt.gca().set_title('Linear Regression')\n",
    "plt.gca().set_ylabel('Independent variable')\n",
    "plt.gca().set_xlabel('Dependent variable');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OLSfit = sm.OLS(y, sm.add_constant(x, prepend=True)).fit()\n",
    "print(OLSfit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive fields of cortical area M1 (MI) neurons [Truccolo et al. 2004]\n",
    "### Smooth pursuit task\n",
    "<img src=\"http://jn.physiology.org/content/jn/113/2/487/F1.medium.gif\", width=300>\n",
    "<div style=\"text-align: right\"> [Addou et al, J. Neurophysiology 2015] </div>\n",
    "\n",
    "\n",
    "### Tuning curves/receptive fields extracted using GLMs:\n",
    "<img src=\"VelocityTuningCurves.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatio-temporal activity of a network of retinal neurons [Pillow et al. 2008]\n",
    "<img src=\"RetinaConnectivity.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the trigeminal ganglion (Vg) neurons code for? [Bush et al. 2016]\n",
    "<img src=\"MouseWhiskerDeflection.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Neuron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a Poisson model:\n",
    "\n",
    "$$p(y|\\lambda)=\\frac{(\\lambda\\Delta)^y}{y!}\\exp(-\\lambda\\Delta)$$\n",
    "\n",
    "* $\\lambda\\Delta$ is the expected number of spikes in time interval $\\Delta$\n",
    "* $\\lambda$ is the (time dependent) intensity of the Poisson process\n",
    "\n",
    "Linear model for expectation value $E(y)$:\n",
    "$$E(y|\\vec{\\theta}) = f(\\vec{\\theta}\\cdot\\vec{x})$$\n",
    "\n",
    "For Poisson distribution we have:\n",
    "* $E(y|\\vec{\\theta})=\\lambda(\\vec{\\theta})$\n",
    "* $f(\\cdot)=\\exp(\\cdot)$\n",
    "\n",
    "$$\\lambda(\\vec{\\theta}) = \\exp(\\vec{\\theta}\\cdot\\vec{x})$$\n",
    "\n",
    "How about other distributions? $\\rightarrow$ <a href=\"https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function\">Wikipedia article</a>\n",
    "\n",
    "### Optimal parameters?\n",
    "Likelihood to see a spike train $Y=\\{y_k\\}_{k=0,\\ldots,T}$ given the parameters $\\vec{\\theta}$\n",
    "\n",
    "$$p(Y|\\vec{\\theta})=\\prod_{k=1}^T\\frac{(\\lambda_k(\\vec{\\theta})\\Delta)^{y_k}}{y_k!}\\exp(-\\lambda_k(\\vec{\\theta})\\Delta)$$\n",
    "\n",
    "and the log likelihood $\\mathcal{L}(\\vec{\\theta})\\equiv\\log p(Y|\\vec{\\theta})$:\n",
    "\n",
    "$$\\mathcal{L}(\\vec{\\theta})=\\sum_k y_k\\log\\lambda_k(\\vec{\\theta}) + y_k\\log\\Delta -\\log y_k! -\\lambda_k(\\vec{\\theta})\\Delta$$\n",
    "collecting parameter independent terms in a constant $c$:\n",
    "$$\\mathcal{L}(\\vec{\\theta})=\\sum_{k} y_k\\log\\lambda_k(\\vec{\\theta}) -\\Delta\\sum_k\\lambda_k(\\vec{\\theta}) + c$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient ascent\n",
    "\n",
    "Iterative optimization procedure:\n",
    "$$\\vec{\\theta}_{n+1} = \\vec{\\theta}_{n} - H^{-1}\\nabla\\mathcal{L}(\\vec{\\theta}_{n}) $$\n",
    "\n",
    "With gradient $\\nabla\\mathcal{L}(\\vec{\\theta})=\\sum_{k} y_k\\vec{x}_{k} -\\Delta\\sum_k\\vec{x}_k\\lambda_k(\\vec{\\theta})$ and Hessian $H=-\\Delta\\sum_k\\vec{x}_k\\vec{x}_k^T\\lambda_k(\\vec{\\theta})$\n",
    "\n",
    "### Giving parameters a meaning\n",
    "\n",
    "<img src=\"GLM_flowdiagram.png\">\n",
    "\n",
    "$$\\lambda_k = \\exp(\\vec{h}\\cdot\\vec{y}_{k,\\tau}+\\vec{a}\\cdot\\vec{x}_{k,\\tau} + a_0)\\text{   with   } \\vec{y}_{k,\\tau}=\\{y_i\\}_{i=k-1,\\ldots,k-\\tau}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# produce artificial data\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "\n",
    "# function to compute the instantaneous firing rate lambda\n",
    "def lam(a,i,N,k,taum,Y):\n",
    "\ts=0  # sum\n",
    "\tfor mi in range (0,taum):\n",
    "\t\tfor ji in range (0,N+1):\n",
    "\t\t\ts = s + a[i,ji,mi]*Y[ji,k-mi-1]\n",
    "\treturn np.exp(s)\n",
    "\n",
    "# function to produce the artificial spike pattern\n",
    "def art_spikedata(a,N,kmax,taum):\n",
    "\tY = np.zeros(shape=(N+1,kmax+taum))\n",
    "\tY[:N,0:taum]=(np.random.poisson(0.1, N*taum).reshape(N,taum)) \n",
    "\tY[N,:] = 1    # weight of the a-zero contribution ([a[:,N,0])\n",
    "\tfor k in range(taum,kmax+taum):\n",
    "\t\tfor i in range(N):\n",
    "\t\t\tY[i,k]=np.random.poisson(lam(a, i, N, k, taum, Y),1)\n",
    "\treturn Y\n",
    "\n",
    "# definition of the parameters\n",
    "N=3\n",
    "taum=20\n",
    "kmax=50000\n",
    "\n",
    "# define parameters for generating the artificial spike train.\n",
    "# i) neuron 0 influences neuron 2 following a cosine kernel.\n",
    "# ii) neuron 1 influences neuron 2 following a sine kernel.\n",
    "# iii) self-interactions follow exponentially decaying cosine.\n",
    "a = np.zeros(shape=(N,N+1,taum))\n",
    "for m in range(taum):\n",
    "\ta[2,0,m] = 0.05 * np.cos((np.pi/2.*m)/taum)  # 0 -> 2\n",
    "\ta[2,1,m] = 0.1 * np.sin((2*np.pi*m)/taum)    # 1 -> 2\n",
    "\tfor i in range(N):   # self-interaction\n",
    "\t\ta[i,i,m] = -0.2*(i+1)/N * np.cos(2*(np.pi*m)/taum) * np.exp(-2*m/taum) #-(0.01*taum-0.01*m)\n",
    "a[:,N,0] = 0.1   # offset for all neurons\n",
    "\n",
    "# Generation of the artificial spike train\n",
    "Y = art_spikedata(a, N, kmax, taum)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definition of the spike number in each sliding window (vecorized) of dimension (N+1)*taum for every k[0:kmax]. In order to establish back the contribution of a[N+1,0] (wich correspond to the mathematical alpha(i,0)) we set all spike count to zero apart from one every taum\n",
    "X = np.zeros(shape=[kmax,(N+1)*taum])\n",
    "for k in range(kmax):\n",
    "\tX[k,:] = np.reshape(Y[:,k:k+taum],(N+1)*taum)\n",
    "\tX[k,N*taum:(N+1)*taum-1] = 0\n",
    "\n",
    "#formulation of GLM\n",
    "alpha = np.zeros(shape=[N,(N+1)*taum])\n",
    "for i in range(N):\n",
    "\tmodel = sm.GLM(Y[i,taum:],X,family=sm.families.Poisson())\n",
    "\talpha[i,:] = model.fit().params\n",
    "\n",
    "#reshaping of the result in order to make them easily comparable with the expected results\n",
    "alphaReshape=alpha.reshape(N,N+1,taum) #create a tensor\n",
    "aR=alphaReshape[:,:,::-1] #inverting the order of element in row\n",
    "# put alpha_i,0 to the beginning of the array rather than the end, more convenient for plotting\n",
    "a = a[:,[-1]+np.arange(N),:] # using advanced slicing to re-order columns\n",
    "aR = aR[:,[-1]+np.arange(N),:] # using advanced slicing to re-order columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot of result and of the expected result\n",
    "fig= pl.figure()\n",
    "x=np.arange(taum)    #definition of axes x\n",
    "for i in range(N):   #definition of axes y\n",
    "\tfor j in range(N):\n",
    "\t\tpl.plot(x,aR[i,j,:], '*', c='r')\n",
    "\t\tpl.plot(x,a[i,j,:], c='b')\n",
    "\t\tpl.ylim((-0.2,0.2))\n",
    "\t\tpl.xlabel(r'time [a.u.]',fontsize=17)\n",
    "\t\tpl.ylabel(r'$\\alpha_{%d,%d}$ [a.u.]' % (i+1,j),fontsize=17)\n",
    "\t\tpl.savefig('alpha%d_%d.png' % (i+1,j))\n",
    "\t\tpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
